{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Knowledge Base RAG\n",
    "\n",
    "Code for knowledge base RAG built with Pinecone, Canopy and OpenAI. The goal is to use the code in this notebook directly in the research assistant project without much refactoring.\n",
    "\n",
    "The knowledge base RAG will support the following functionalities:\n",
    "- Upload: Upload a document in the canopy format to the Pinecone index.\n",
    "- Chat: Chat with the knowledge base to get answers to questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = ...\n",
    "os.environ[\"OPENAI_API_KEY\"] = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading documents\n",
    "\n",
    "The documents are uploaded on demand to the Pinecone index. \n",
    "\n",
    "The document should have the following attributes:\n",
    "- id: unique identifier for the document\n",
    "- text: the text of the document, in utf-8 encoding.\n",
    "- source: the source of the document, can be any string, or null. This will be used as a reference in the generated context.\n",
    "- metadata: optional metadata for the document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Init Tokenizer\n",
    "\n",
    "Many of Canopy's components are using tokenization, which is a process that splits text into tokens - basic units of text (like word or sub-words) that are used for processing. Therefore, Canopy uses a singleton Tokenizer object which needs to be initialized once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.tokenizer import Tokenizer\n",
    "Tokenizer.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Pinecone index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.knowledge_base import KnowledgeBase\n",
    "from canopy.knowledge_base import list_canopy_indexes\n",
    "\n",
    "# canopy prefixes index names with \"canopy--\"\n",
    "INDEX_NAME = \"knowledge-base\"\n",
    "\n",
    "kb = KnowledgeBase(index_name=INDEX_NAME)\n",
    "\n",
    "if not any(name.endswith(INDEX_NAME) for name in list_canopy_indexes()):\n",
    "    kb.create_canopy_index()\n",
    "\n",
    "kb.connect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Upload document to index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will upload the paper \"Mixtral of Experts\" to the Pinecone index. The paper is available in the `example-document.json` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['id', 'source', 'text', 'metadata'])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "document = json.load(open('example-document.json'))\n",
    "document.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the json document to a canopy document object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2401.04088'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from canopy.models.data_models import Document\n",
    "\n",
    "document = Document(**document)\n",
    "document.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the document object to the index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload(document):\n",
    "    # upload a single document to the knowledge base\n",
    "    return kb.upsert([document])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat with the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.chat_engine import ChatEngine\n",
    "from canopy.context_engine import ContextEngine\n",
    "\n",
    "context_engine = ContextEngine(kb)\n",
    "chat_engine = ChatEngine(context_engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from canopy.models.data_models import UserMessage, AssistantMessage\n",
    "\n",
    "def chat(new_message, history):\n",
    "    messages = history + [UserMessage(content=new_message)]\n",
    "    response = chat_engine.chat(messages)\n",
    "    assistant_response = response.choices[0].message.content\n",
    "    return assistant_response, messages + [AssistantMessage(content=assistant_response)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The main ideas presented in the paper \"Mixtral of Experts\" include introducing Mixtral as a sparse mixture-of-experts model with a fully dense context length of 32k tokens. This model architecture has a decoder-only design with a sparse mixture-of-experts network that uses a subset of its parameters for every token, allowing for faster inference speeds at low batch sizes and higher throughput at large batch sizes. Mixtral utilizes a mechanism where a router network chooses two expert groups to process each token, which enables the model to control cost and latency effectively. Additionally, Mixtral is shown to outperform other models like Llama 2 70B and GPT-3.5 on various benchmarks, especially excelling in mathematics, code generation, and multilingual tasks. The model also uses significantly fewer active parameters per token while maintaining high performance compared to models with a higher number of parameters per token. Furthermore, Mixtral was pretrained with multilingual data and is made publicly available under the Apache 2.0 license to facilitate further research and application development in various industries and domains.\n",
      "Sources: http://arxiv.org/pdf/2401.04088\n"
     ]
    }
   ],
   "source": [
    "history = []\n",
    "\n",
    "response, history = chat(\"What are main ideas presented in the paper \\\"Mixtral of Expert\\\"?\", history)\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
